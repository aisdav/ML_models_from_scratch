{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "df4f8a4c-befd-4420-bf46-0c633a2519b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import (\n",
    "    make_regression,\n",
    "    make_classification,\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, mean_absolute_error,mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2d9b40-0681-4b11-aa16-0a0565e6e4fd",
   "metadata": {},
   "source": [
    "# Our model Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "de4e1fbe-8e26-4840-8bcb-32434f9af2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest:\n",
    "    def __init__(self, regression=False, n_estimators=100, max_depth=None,\n",
    "                 max_features=1.0, n_jobs=-1, random_state=0, ccp_alpha=0.0):\n",
    "        self.regression = regression\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.max_features = max_features\n",
    "        self.n_jobs = n_jobs\n",
    "        self.random_state = random_state\n",
    "        self.ccp_alpha = ccp_alpha\n",
    "        self.trained_trees_info = []\n",
    "        self.general_random = np.random.RandomState(self.random_state)\n",
    "\n",
    "    #bootstrapping with random subspaces method\n",
    "    def _rsm_bootstrapping(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        if self.regression:\n",
    "            max_features = self.max_features * n_features\n",
    "        else:\n",
    "            max_features = np.sqrt(n_features)\n",
    "\n",
    "        sample_indexes = self.general_random.choice(n_samples, n_samples)\n",
    "        features = self.general_random.choice(X.columns, round(max_features))\n",
    "        X_b, y_b = X.iloc[sample_indexes][features], y.iloc[sample_indexes]\n",
    "\n",
    "        return X_b, y_b\n",
    "\n",
    "    def _train_tree(self, X, y):\n",
    "        if self.regression:\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth,\n",
    "                                         random_state=self.random_state,\n",
    "                                         ccp_alpha=self.ccp_alpha)\n",
    "        else:\n",
    "            tree = DecisionTreeClassifier(max_depth=self.max_depth,\n",
    "                                          random_state=self.random_state,\n",
    "                                          ccp_alpha=self.ccp_alpha)\n",
    "\n",
    "        return tree.fit(X, y), X.columns\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        boot_data = (self._rsm_bootstrapping(X, y) for _ in range(self.n_estimators))\n",
    "        train_trees = (delayed(self._train_tree)(X_b, y_b) for X_b, y_b in boot_data)\n",
    "        self.trained_trees_info = Parallel(n_jobs=self.n_jobs)(train_trees)\n",
    "\n",
    "    def predict(self, samples):\n",
    "        prediction = (delayed(tree_i.predict)(samples[tree_i_features])\n",
    "                      for (tree_i, tree_i_features) in self.trained_trees_info)\n",
    "\n",
    "        trees_predictions = pd.DataFrame(Parallel(n_jobs=self.n_jobs)(prediction))\n",
    "\n",
    "        if self.regression:\n",
    "            forest_prediction = trees_predictions.mean(axis=0)\n",
    "        else:\n",
    "            forest_prediction = trees_predictions.mode(axis=0).iloc[0]\n",
    "\n",
    "        return np.array(forest_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b0fbd2-3ec8-47d4-a2c5-d2aba11effd1",
   "metadata": {},
   "source": [
    "# Dataset for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f4abb955-7f5b-4892-ac27-cade7c833440",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_clsf, y_clsf = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=10,\n",
    "    n_informative=5,\n",
    "    n_redundant=2,\n",
    "    n_classes=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X_clsf = pd.DataFrame(X_clsf, columns=[f\"f{i}\" for i in range(X_clsf.shape[1])])\n",
    "y_clsf = pd.Series(y_clsf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e241f4-5ad7-4dde-8371-5df96827f143",
   "metadata": {},
   "source": [
    "# Dataset for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8c5d91d4-8c3a-46be-b2e8-87b8189d0255",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "\n",
    "X_reg, y_reg = make_regression(\n",
    "    n_samples=1000,\n",
    "    n_features=10,\n",
    "    n_informative=5,\n",
    "    noise=10,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X_reg = pd.DataFrame(X_reg, columns=[f\"f{i}\" for i in range(X_reg.shape[1])])\n",
    "y_reg = pd.Series(y_reg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f236b6f3-843a-4f7e-b8e7-702ad8125652",
   "metadata": {},
   "source": [
    "# Noisy classification dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "35331433-afb6-4213-b1e9-66efd8e7a1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_noisy, y_noisy = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=20,\n",
    "    n_informative=2,\n",
    "    n_redundant=10,\n",
    "    flip_y=0.25,\n",
    "    class_sep=0.5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X_noisy = pd.DataFrame(X_noisy, columns=[f\"f{i}\" for i in range(X_noisy.shape[1])])\n",
    "y_noisy = pd.Series(y_noisy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a6895d-60df-4869-b5a2-b39c2f302a86",
   "metadata": {},
   "source": [
    "# How number of estimators affects on metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a198271f-dcd1-43c8-9bc8-71f0e534d3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.5466666666666666\n",
      "5 0.74\n",
      "10 0.81\n",
      "50 0.8733333333333333\n",
      "100 0.8866666666666667\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_clsf, y_clsf, test_size=0.3)\n",
    "for n in [1,5,10,50,100]:\n",
    "    rf = RandomForest(n_estimators=n)\n",
    "    rf.fit(X_train,y_train)\n",
    "    preds = rf.predict(X_test)\n",
    "    print(n,accuracy_score(y_test,preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe23956-f9c3-4292-af51-317ab19c4f7b",
   "metadata": {},
   "source": [
    "## At 10 estimators there is a jump from 5 estimators. After 10 probably there is overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780b744c-c17c-4a90-8e65-a678216c3fbf",
   "metadata": {},
   "source": [
    "# How max depth insfluence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "58978732-ee82-4cca-b2bf-7714f4d86c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 0.8133333333333334\n",
      "4 0.8633333333333333\n",
      "8 0.8633333333333333\n",
      "None 0.8533333333333334\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_clsf, y_clsf, test_size=0.3)\n",
    "for depth in [2,4,8,None]:\n",
    "    rf = RandomForest(n_estimators=50,max_depth=depth)\n",
    "    rf.fit(X_train,y_train)\n",
    "    preds = rf.predict(X_test)\n",
    "    print(depth,accuracy_score(y_test,preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e49fc3d-f8c2-43cd-a00c-33655d696a5b",
   "metadata": {},
   "source": [
    "## Starting from 4 depth metrics dont change significantly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1619f4-6e15-486a-9d13-b21209b03199",
   "metadata": {},
   "source": [
    "# How parametr max feature effect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8badac12-281c-45e5-8b49-5744ce45bcfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3 0.6966666666666667\n",
      "0.5 0.6966666666666667\n",
      "1.0 0.6966666666666667\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_noisy, y_noisy, test_size=0.3)\n",
    "for mf in [0.3, 0.5, 1.0]:\n",
    "    rf = RandomForest(n_estimators=50, max_features=mf)\n",
    "    rf.fit(X_train, y_train)\n",
    "    preds = rf.predict(X_test)\n",
    "    print(mf, accuracy_score(y_test, preds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf65a130-c713-4bfe-8581-9199f0d88a81",
   "metadata": {},
   "source": [
    "## In the third experiment, changing the max_features parameter did not lead to noticeable changes in quality metrics. This is due to the specifics of the algorithm implementation: for the classification task, the number of features used is fixed and does not depend on the value of max_features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a9438c-59ad-4420-b0cf-c608e946db43",
   "metadata": {},
   "source": [
    "# Comparing RF with single DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c9e8824e-8ce2-4eaf-8b38-b8b4ed8ba48c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree: 0.6666666666666666\n",
      "RF: 0.73\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_noisy, y_noisy, test_size=0.3)\n",
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(X_train, y_train)\n",
    "dt_preds = dt.predict(X_test)\n",
    "\n",
    "rf = RandomForest(n_estimators=50)\n",
    "rf.fit(X_train, y_train)\n",
    "rf_preds = rf.predict(X_test)\n",
    "\n",
    "print(\"Tree:\", accuracy_score(y_test, dt_preds))\n",
    "print(\"RF:\", accuracy_score(y_test, rf_preds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedf6cfe-c591-4c93-9ffb-5805e7e9a27c",
   "metadata": {},
   "source": [
    "## We can see the difference between them. RF more stable because of number of tree. It reduces risk to overfit and variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7cf943-5156-4f46-bac6-6e9c8c27e97d",
   "metadata": {},
   "source": [
    "# How RF handle noisy data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "de4bd0f0-f0f5-4663-90cc-f9b201110d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.865\n",
      "std: 0.028057480682025293\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_clsf, y_clsf, test_size=0.3)\n",
    "scores = []\n",
    "for seed in range(10):\n",
    "    rf = RandomForest(n_estimators=30, random_state=seed)\n",
    "    rf.fit(X_train, y_train)\n",
    "    preds = rf.predict(X_test)\n",
    "    scores.append(accuracy_score(y_test, preds))\n",
    "\n",
    "print(\"mean:\", np.mean(scores))\n",
    "print(\"std:\", np.std(scores))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6799662d-5d3f-4249-a2dc-ec28ba524d0c",
   "metadata": {},
   "source": [
    "## RF has 0.865 average accuracy which is pretty well. While std is 0.02 showing its stability for random state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4193c96f-bd18-447d-a0e8-7722d98eb92c",
   "metadata": {},
   "source": [
    "# Comparing RF and DT in regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d1482cad-957e-4ad0-9e94-4709648fa4a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF MAE: 23.76913003739627\n",
      "RF MSE: 873.2022438193327\n",
      "DT MAE: 22.135846779935033\n",
      "DT MSE: 774.2636379055491\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_reg, y_reg, test_size=0.3)\n",
    "rf = RandomForest(\n",
    "    regression=True,\n",
    "    n_estimators=50,\n",
    "    max_depth=None,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "rf_preds = rf.predict(X_test)\n",
    "\n",
    "print(\"RF MAE:\", mean_absolute_error(y_test, rf_preds))\n",
    "print(\"RF MSE:\", mean_squared_error(y_test, rf_preds))\n",
    "\n",
    "dt = DecisionTreeRegressor(random_state=42)\n",
    "dt.fit(X_train, y_train)\n",
    "dt_preds = dt.predict(X_test)\n",
    "\n",
    "print(\"DT MAE:\", mean_absolute_error(y_test, dt_preds))\n",
    "print(\"DT MSE:\", mean_squared_error(y_test, dt_preds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637350f8-ecb8-423b-980b-66ced1e67fcc",
   "metadata": {},
   "source": [
    "## Metrics shows RF with this parameters is worse than DT. Bias in RF increase, although variance reduced. Rf doesnt guarantee better metrics without proper parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0140b34c-9098-470c-8dec-ee70d2c74f5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
